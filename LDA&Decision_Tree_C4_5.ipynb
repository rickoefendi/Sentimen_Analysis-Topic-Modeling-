{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyyE7bjvfGBs/krSIkgUb1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickoefendi/Sentimen_Analysis-Topic-Modeling-/blob/main/LDA%26Decision_Tree_C4_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREPOCESSING**"
      ],
      "metadata": {
        "id": "-q9sBRYVD4pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources (run this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('data_Riko.csv')\n",
        "\n",
        "# Initialize WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define stop words\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "    # Join tokens back to string\n",
        "    cleaned_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply preprocessing to the text column\n",
        "data['cleaned_text'] = data['full_text'].apply(preprocess)\n",
        "\n",
        "# Save the preprocessed data to a new CSV file\n",
        "data.to_csv('cleaned_data_wisata.csv', index=False)\n",
        "\n",
        "print(\"Preprocessing completed. Cleaned data saved to 'cleaned_data_wisata.csv'.\")\n"
      ],
      "metadata": {
        "id": "WX2OcFhjEFwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LABELING**"
      ],
      "metadata": {
        "id": "CzRBsXpmD_Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Download VADER lexicon (run this once)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('cleaned_data_wisata.csv')\n",
        "\n",
        "# Initialize VADER SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment_labeling(text):\n",
        "    # Periksa apakah teks adalah string, jika tidak, ubah menjadi string kosong\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\"\n",
        "\n",
        "    # Dapatkan skor sentimen dari VADER\n",
        "    sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "    # Tentukan label sentimen berdasarkan skor compound\n",
        "    if sentiment_scores['compound'] >= 0.05:\n",
        "        return \"positive\"\n",
        "    elif sentiment_scores['compound'] <= -0.05:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "# Terapkan fungsi vader_sentiment_labeling ke kolom teks yang telah dibersihkan\n",
        "data['sentiment'] = data['cleaned_text'].apply(vader_sentiment_labeling)\n",
        "\n",
        "# Hapus entri dengan label netral\n",
        "filtered_data = data[data['sentiment'] != 'neutral']\n",
        "\n",
        "# Simpan data yang sudah dilabel ke file CSV baru\n",
        "filtered_data.to_csv('filtered_labeled_data_wisata.csv', index=False)\n",
        "\n",
        "print(\"Labeling completed. Filtered labeled data saved to 'filtered_labeled_data_wisata.csv'.\")\n",
        "\n",
        "# Visualisasi distribusi sentimen (tanpa kelas netral)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='sentiment', data=filtered_data, palette='viridis')\n",
        "plt.title('Distribusi Sentimen (Positif vs Negatif)')\n",
        "plt.xlabel('Sentimen')\n",
        "plt.ylabel('Jumlah')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "burNO8zAEN1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iU6Iu-FaEaI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Laten Dirichlet Allocation**"
      ],
      "metadata": {
        "id": "2Iblhpg8EYXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim pyLDAvis matplotlib"
      ],
      "metadata": {
        "id": "01ZXwFcrEf1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Load preprocessed data\n",
        "data = pd.read_csv('cleaned_data_wisata.csv')\n",
        "\n",
        "# Pastikan kolom cleaned_text adalah string\n",
        "data['cleaned_text'] = data['cleaned_text'].astype(str)\n",
        "\n",
        "# Hapus baris dengan teks kosong\n",
        "data = data[data['cleaned_text'].str.strip().astype(bool)]\n",
        "\n",
        "# Tokenisasi teks\n",
        "tokenized_texts = [text.split() for text in data['cleaned_text']]\n",
        "\n",
        "# Membuat dictionary dan corpus\n",
        "dictionary = corpora.Dictionary(tokenized_texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
        "\n",
        "# Model LDA\n",
        "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, random_state=42)\n",
        "\n",
        "# Menyimpan model LDA\n",
        "lda_model.save('lda_model_wisata.gensim')\n",
        "\n",
        "# Menyimpan dictionary\n",
        "dictionary.save('dictionary.dict')\n",
        "\n",
        "# Visualisasi dengan pyLDAvis\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "\n",
        "# Menyimpan visualisasi sebagai HTML\n",
        "pyLDAvis.save_html(vis, 'lda_visualization.html')\n",
        "print(\"LDA visualization saved as 'lda_visualization.html'.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TsDCQoN2ElF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan kata-kata teratas untuk setiap topik\n",
        "topics = lda_model.print_topics(num_words=10)\n",
        "print(\"Topik-topik yang dihasilkan:\")\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "id": "ZAAjmhx1Ek90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisasi dengan pyLDAvis\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "\n",
        "# Menampilkan visualisasi di output (langsung tanpa menyimpan)\n",
        "pyLDAvis.display(vis)"
      ],
      "metadata": {
        "id": "wRGm1xnVEy3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Menghitung koherensi topik\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_texts, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "\n",
        "print(f\"Koherensi Model LDA: {coherence_lda:.4f}\")"
      ],
      "metadata": {
        "id": "iao25kiKE5BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree**"
      ],
      "metadata": {
        "id": "ACYeTWdRE_WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "# Daftar stop-words bahasa Indonesia\n",
        "stop_words_id = [\n",
        "    'akan', 'adalah', 'aku', 'apa', 'apakah', 'atau', 'dalam', 'dan', 'di', 'untuk',\n",
        "    'dengan', 'ini', 'itu', 'jika', 'kamu', 'kami', 'kita', 'kita', 'mereka', 'saya',\n",
        "    'tidak', 'yang', 'oleh', 'sebagai', 'dari', 'ke', 'pada', 'oleh', 'adalah', 'dari'\n",
        "]\n",
        "\n",
        "# Load labeled data\n",
        "data = pd.read_csv('filtered_labeled_data_wisata.csv')\n",
        "\n",
        "# Feature Extraction dengan stop-words bahasa Indonesia\n",
        "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=stop_words_id)\n",
        "X = vectorizer.fit_transform(data['cleaned_text'])\n",
        "y = data['sentiment']\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier dengan membatasi kedalaman\n",
        "max_depth = 3  # Atur kedalaman maksimum pohon\n",
        "clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and Evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluasi Model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "# Menampilkan hasil prediksi per kelas\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the trained model if needed\n",
        "joblib.dump(clf, 'decision_tree_model.pkl')\n",
        "\n",
        "# Visualisasi Pohon Keputusan\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(clf,\n",
        "          feature_names=vectorizer.get_feature_names_out(),\n",
        "          class_names=clf.classes_,\n",
        "          filled=True,\n",
        "          fontsize=10)\n",
        "plt.title(\"Decision Tree Visualization\")\n",
        "\n",
        "# Simpan visualisasi sebagai PDF\n",
        "plt.savefig('decision_tree_visualization.pdf')\n",
        "\n",
        "print(\"Decision Tree visualization saved as 'decision_tree_visualization.pdf'.\")\n"
      ],
      "metadata": {
        "id": "3OhvLPTVE9sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from gensim import corpora\n",
        "\n",
        "# Load the LDA model and dictionary\n",
        "lda_model = LdaModel.load('lda_model_wisata.gensim')\n",
        "dictionary = corpora.Dictionary.load('dictionary.dict')\n",
        "\n",
        "# Visualisasi Word Clouds untuk setiap topik\n",
        "num_topics = lda_model.num_topics\n",
        "for i in range(num_topics):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    # Dapatkan kata dan bobot dari topik\n",
        "    words = dict(lda_model.show_topic(i, 20))  # 20 kata utama per topik\n",
        "\n",
        "    # Buat Word Cloud\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(words)\n",
        "\n",
        "    # Tampilkan Word Cloud\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Topic {i+1}')\n",
        "\n",
        "    # Simpan Word Cloud sebagai file PNG\n",
        "    plt.savefig(f'topic_{i+1}_wordcloud.png')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Word clouds saved as 'topic_X_wordcloud.png'.\")\n"
      ],
      "metadata": {
        "id": "u-9VQNVTFI4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load labeled data\n",
        "data = pd.read_csv('filtered_labeled_data_wisata.csv')\n",
        "\n",
        "# Tokenisasi teks untuk membuat corpus\n",
        "tokenized_texts = [text.split() for text in data['cleaned_text']]\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
        "\n",
        "# Tentukan topik untuk setiap dokumen\n",
        "topic_distribution = [lda_model.get_document_topics(doc) for doc in corpus]\n",
        "\n",
        "# Tambahkan topik dominan untuk setiap dokumen ke dalam data\n",
        "def get_dominant_topic(topics):\n",
        "    if not topics:\n",
        "        return None\n",
        "    return max(topics, key=lambda x: x[1])[0]\n",
        "\n",
        "data['dominant_topic'] = [get_dominant_topic(doc_topics) for doc_topics in topic_distribution]\n",
        "\n",
        "# Hitung distribusi sentimen per topik\n",
        "sentiment_per_topic = data.groupby(['dominant_topic', 'sentiment']).size().unstack(fill_value=0)\n",
        "\n",
        "print(\"Sentiment distribution per topic:\")\n",
        "print(sentiment_per_topic)\n",
        "\n",
        "# Visualisasi distribusi sentimen per topik dengan bar chart\n",
        "sentiment_per_topic.plot(kind='bar', figsize=(8, 4))\n",
        "plt.title('Sentiment Distribution per Topic')\n",
        "plt.xlabel('Topic')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(ticks=np.arange(len(sentiment_per_topic.index)), labels=[f'Topic {i+1}' for i in sentiment_per_topic.index], rotation=45)\n",
        "plt.legend(title='Sentiment')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Simpan visualisasi distribusi sentimen sebagai file PNG\n",
        "plt.savefig('sentiment_distribution_per_topic.png')\n",
        "plt.show()\n",
        "\n",
        "print(\"Sentiment distribution chart saved as 'sentiment_distribution_per_topic.png'.\")\n"
      ],
      "metadata": {
        "id": "WgZ1XhXPFSil"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}